{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec-x1EfUOFY2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install numpyro==0.10.1\n",
        "%pip install optax\n",
        "%pip install -U jax jaxlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3c_4DRONw3",
        "outputId": "cd301810-cc34-4ba1-b1f2-f756b70978d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'tbip'...\n",
            "remote: Enumerating objects: 226, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 226 (delta 47), reused 42 (delta 39), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (226/226), 61.76 MiB | 14.84 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/rakoort99/TBIP_testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZjX1Cv7OV0U",
        "outputId": "cc17aa7a-e8a0-4739-a05e-f9bf8809df13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "from jax import random\n",
        "\n",
        "num_topics = 10\n",
        "rng_seed = random.PRNGKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "EjHpfwsN7UiT",
        "outputId": "2e42f130-0d40-4cb0-e3b2-55b0a6ceb40c"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e2db0afa7acf>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m author_indices = jax.device_put(\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"author_indices.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mdevices\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mDevice\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m   \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0mplatform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m ) -> xla_client.Client:\n\u001b[0;32m--> 692\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mplatform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_backends_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         raise RuntimeError(f\"Backend '{platform}' failed to initialize: \"\n\u001b[0m\u001b[1;32m    680\u001b[0m                            f\"{_backends_errors[platform]}\")\n\u001b[1;32m    681\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown backend {platform}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Backend 'tpu' failed to initialize: INVALID_ARGUMENT: TpuPlatform is not available."
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "dataPath = \"TBIP_testing/data/paradigms/clean/\"\n",
        "\n",
        "# Load data\n",
        "author_indices = jax.device_put(\n",
        "    jnp.load(dataPath + \"author_indices.npy\"), jax.devices(\"gpu\")[0]\n",
        ")\n",
        "\n",
        "counts = sparse.load_npz(dataPath + \"counts.npz\")\n",
        "\n",
        "with open(dataPath + \"vocabulary.txt\", \"r\") as f:\n",
        "    vocabulary = f.readlines()\n",
        "\n",
        "with open(dataPath + \"author_map.txt\", \"r\") as f:\n",
        "    author_map = f.readlines()\n",
        "\n",
        "author_map = np.array(author_map)\n",
        "\n",
        "num_authors = int(author_indices.max() + 1)\n",
        "num_documents, num_words = counts.shape\n",
        "\n",
        "print('num_docs:', num_documents)\n",
        "print('num_authors', num_authors)\n",
        "print('dim authormap', author_map.shape)\n",
        "print('dim author indices', author_indices.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUYk1s3778xa"
      },
      "outputs": [],
      "source": [
        "pre_initialize_parameters = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzAVOWQX79JU"
      },
      "outputs": [],
      "source": [
        "# Fit NMF to be used as initialization for TBIP\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "if pre_initialize_parameters:\n",
        "    nmf_model = NMF(\n",
        "        n_components=num_topics, init=\"random\", random_state=0, max_iter=500\n",
        "    )\n",
        "    # Define initialization arrays\n",
        "    initial_document_loc = jnp.log(\n",
        "        jnp.array(np.float32(nmf_model.fit_transform(counts) + 1e-2))\n",
        "    )\n",
        "    initial_objective_topic_loc = jnp.log(\n",
        "        jnp.array(np.float32(nmf_model.components_ + 1e-2))\n",
        "    )\n",
        "else:\n",
        "    rng1, rng2 = random.split(rng_seed, 2)\n",
        "    initial_document_loc = random.normal(rng1, shape=(num_documents, num_topics))\n",
        "    initial_objective_topic_loc = random.normal(rng2, shape=(num_topics, num_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iujK2LZJ9HUV"
      },
      "outputs": [],
      "source": [
        "from numpyro import plate, sample, param\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.distributions import constraints\n",
        "\n",
        "# Define the model and variational family\n",
        "\n",
        "\n",
        "class TBIP:\n",
        "    def __init__(self, N, D, K, V, batch_size, init_mu_theta=None, init_mu_beta=None):\n",
        "        self.N = N  # number of people\n",
        "        self.D = D  # number of documents\n",
        "        self.K = K  # number of topics\n",
        "        self.V = V  # number of words in vocabulary\n",
        "        self.batch_size = batch_size  # number of documents in a batch\n",
        "\n",
        "        if init_mu_theta is None:\n",
        "            init_mu_theta = jnp.zeros([D, K])\n",
        "        else:\n",
        "            self.init_mu_theta = init_mu_theta\n",
        "\n",
        "        if init_mu_beta is None:\n",
        "            init_mu_beta = jnp.zeros([K, V])\n",
        "        else:\n",
        "            self.init_mu_beta = init_mu_beta\n",
        "\n",
        "    def model(self, Y_batch, d_batch, i_batch):\n",
        "        with plate(\"i\", self.N):\n",
        "            # Sample the per-unit latent variables (ideal points)\n",
        "            x = sample(\"x\", dist.Normal())\n",
        "\n",
        "        with plate(\"k\", size=self.K, dim=-2):\n",
        "            with plate(\"k_v\", size=self.V, dim=-1):\n",
        "                beta = sample(\"beta\", dist.Gamma(0.3, 0.3))\n",
        "                eta = sample(\"eta\", dist.Normal())\n",
        "\n",
        "        with plate(\"d\", size=self.D, subsample_size=self.batch_size, dim=-2):\n",
        "            with plate(\"d_k\", size=self.K, dim=-1):\n",
        "                # Sample document-level latent variables (topic intensities)\n",
        "                theta = sample(\"theta\", dist.Gamma(0.3, 0.3))\n",
        "\n",
        "            # Compute Poisson rates for each word\n",
        "            P = jnp.sum(\n",
        "                jnp.expand_dims(theta, 2)\n",
        "                * jnp.expand_dims(beta, 0)\n",
        "                * jnp.exp(\n",
        "                    jnp.expand_dims(x[i_batch], (1, 2)) * jnp.expand_dims(eta, 0)\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "\n",
        "            with plate(\"v\", size=self.V, dim=-1):\n",
        "                # Sample observed words\n",
        "                sample(\"Y_batch\", dist.Poisson(P), obs=Y_batch)\n",
        "\n",
        "    def guide(self, Y_batch, d_batch, i_batch):\n",
        "        # This defines variational family. Notice that each of the latent variables\n",
        "        # defined in the sample statements in the model above has a corresponding\n",
        "        # sample statement in the guide. The guide is responsible for providing\n",
        "        # variational parameters for each of these latent variables.\n",
        "\n",
        "        # Also notice it is required that model and the guide have the same call.\n",
        "\n",
        "        mu_x = param(\n",
        "            \"mu_x\", init_value=-1 + 2 * random.uniform(random.PRNGKey(1), (self.N,))\n",
        "        )\n",
        "        sigma_x = param(\n",
        "            \"sigma_y\", init_value=jnp.ones([self.N]), constraint=constraints.positive\n",
        "        )\n",
        "\n",
        "        mu_eta = param(\n",
        "            \"mu_eta\", init_value=random.normal(random.PRNGKey(2), (self.K, self.V))\n",
        "        )\n",
        "        sigma_eta = param(\n",
        "            \"sigma_eta\",\n",
        "            init_value=jnp.ones([self.K, self.V]),\n",
        "            constraint=constraints.positive,\n",
        "        )\n",
        "\n",
        "        mu_theta = param(\"mu_theta\", init_value=self.init_mu_theta)\n",
        "        sigma_theta = param(\n",
        "            \"sigma_theta\",\n",
        "            init_value=jnp.ones([self.D, self.K]),\n",
        "            constraint=constraints.positive,\n",
        "        )\n",
        "\n",
        "        mu_beta = param(\"mu_beta\", init_value=self.init_mu_beta)\n",
        "        sigma_beta = param(\n",
        "            \"sigma_beta\",\n",
        "            init_value=jnp.ones([self.K, self.V]),\n",
        "            constraint=constraints.positive,\n",
        "        )\n",
        "\n",
        "        with plate(\"i\", self.N):\n",
        "            sample(\"x\", dist.Normal(mu_x, sigma_x))\n",
        "\n",
        "        with plate(\"k\", size=self.K, dim=-2):\n",
        "            with plate(\"k_v\", size=self.V, dim=-1):\n",
        "                sample(\"beta\", dist.LogNormal(mu_beta, sigma_beta))\n",
        "                sample(\"eta\", dist.Normal(mu_eta, sigma_eta))\n",
        "\n",
        "        with plate(\"d\", size=self.D, subsample_size=self.batch_size, dim=-2):\n",
        "            with plate(\"d_k\", size=self.K, dim=-1):\n",
        "                sample(\"theta\", dist.LogNormal(mu_theta[d_batch], sigma_theta[d_batch]))\n",
        "\n",
        "    def get_batch(self, rng, Y, author_indices):\n",
        "        # Helper functions to obtain a batch of data, convert from scipy.sparse\n",
        "        # to jax.numpy.array and move to gpu\n",
        "\n",
        "        D_batch = random.choice(rng, jnp.arange(self.D), shape=(self.batch_size,))\n",
        "        Y_batch = jax.device_put(jnp.array(Y[D_batch].toarray()), jax.devices(\"gpu\")[0])\n",
        "        D_batch = jax.device_put(D_batch, jax.devices(\"gpu\")[0])\n",
        "        I_batch = author_indices[D_batch]\n",
        "        return Y_batch, I_batch, D_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL9x0A709iHr"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "from optax import adam, exponential_decay\n",
        "from numpyro.infer import SVI, TraceMeanField_ELBO\n",
        "from jax import jit\n",
        "\n",
        "num_steps = 50000\n",
        "batch_size = 512  # Large batches are recommended\n",
        "learning_rate = 0.01\n",
        "decay_rate = 0.01\n",
        "\n",
        "tbip = TBIP(\n",
        "    N=num_authors,\n",
        "    D=num_documents,\n",
        "    K=num_topics,\n",
        "    V=num_words,\n",
        "    batch_size=batch_size,\n",
        "    init_mu_theta=initial_document_loc,\n",
        "    init_mu_beta=initial_objective_topic_loc,\n",
        ")\n",
        "\n",
        "svi_batch = SVI(\n",
        "    model=tbip.model,\n",
        "    guide=tbip.guide,\n",
        "    optim=adam(exponential_decay(learning_rate, num_steps, decay_rate)),\n",
        "    loss=TraceMeanField_ELBO(),\n",
        ")\n",
        "\n",
        "# Compile update function for faster training\n",
        "svi_batch_update = jit(svi_batch.update)\n",
        "\n",
        "# Get initial batch. This informs the dimension of arrays and ensures they are\n",
        "# consistent with dimensions (N, D, K, V) defined above.\n",
        "Y_batch, I_batch, D_batch = tbip.get_batch(random.PRNGKey(1), counts, author_indices)\n",
        "\n",
        "# Initialize the parameters using initial batch\n",
        "svi_state = svi_batch.init(\n",
        "    random.PRNGKey(0), Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRjKkX2J9_0X"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to create helper function for printing topics\n",
        "\n",
        "\n",
        "def get_topics(\n",
        "    neutral_mean, negative_mean, positive_mean, vocabulary, print_to_terminal=True\n",
        "):\n",
        "    num_topics, num_words = neutral_mean.shape\n",
        "    words_per_topic = 10\n",
        "    top_neutral_words = np.argsort(-neutral_mean, axis=1)\n",
        "    top_negative_words = np.argsort(-negative_mean, axis=1)\n",
        "    top_positive_words = np.argsort(-positive_mean, axis=1)\n",
        "    topic_strings = []\n",
        "    for topic_idx in range(num_topics):\n",
        "        neutral_start_string = \"Neutral  {}:\".format(topic_idx)\n",
        "        neutral_row = [\n",
        "            vocabulary[word] for word in top_neutral_words[topic_idx, :words_per_topic]\n",
        "        ]\n",
        "        neutral_row_string = \", \".join(neutral_row)\n",
        "        neutral_string = \" \".join([neutral_start_string, neutral_row_string])\n",
        "\n",
        "        positive_start_string = \"Positive {}:\".format(topic_idx)\n",
        "        positive_row = [\n",
        "            vocabulary[word] for word in top_positive_words[topic_idx, :words_per_topic]\n",
        "        ]\n",
        "        positive_row_string = \", \".join(positive_row)\n",
        "        positive_string = \" \".join([positive_start_string, positive_row_string])\n",
        "\n",
        "        negative_start_string = \"Negative {}:\".format(topic_idx)\n",
        "        negative_row = [\n",
        "            vocabulary[word] for word in top_negative_words[topic_idx, :words_per_topic]\n",
        "        ]\n",
        "        negative_row_string = \", \".join(negative_row)\n",
        "        negative_string = \" \".join([negative_start_string, negative_row_string])\n",
        "\n",
        "        if print_to_terminal:\n",
        "            topic_strings.append(negative_string)\n",
        "            topic_strings.append(neutral_string)\n",
        "            topic_strings.append(positive_string)\n",
        "            topic_strings.append(\"==========\")\n",
        "        else:\n",
        "            topic_strings.append(\n",
        "                \"  \\n\".join([negative_string, neutral_string, positive_string])\n",
        "            )\n",
        "\n",
        "    if print_to_terminal:\n",
        "        all_topics = \"{}\\n\".format(np.array(topic_strings))\n",
        "    else:\n",
        "        all_topics = np.array(topic_strings)\n",
        "    return all_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "17ZjXgRs-E5D",
        "outputId": "80c511c2-8322-41b1-b1ea-151cac5f3580"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Init loss: 756123328.0000; Avg loss (last 100 iter): 756123328.0000:   0%|          | 53/50000 [01:13<19:19:31,  1.39s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8b56c40b8de2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtbip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrngs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     svi_state, loss = svi_batch_update(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0msvi_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mI_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run SVI\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "print_steps = 100\n",
        "print_intermediate_results = False\n",
        "\n",
        "rngs = random.split(random.PRNGKey(2), num_steps)\n",
        "losses = []\n",
        "pbar = tqdm(range(num_steps))\n",
        "\n",
        "\n",
        "for step in pbar:\n",
        "    Y_batch, I_batch, D_batch = tbip.get_batch(rngs[step], counts, author_indices)\n",
        "    svi_state, loss = svi_batch_update(\n",
        "        svi_state, Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n",
        "    )\n",
        "\n",
        "    loss = loss / counts.shape[0]\n",
        "    losses.append(loss)\n",
        "    if step % print_steps == 0 or step == num_steps - 1:\n",
        "        pbar.set_description(\n",
        "            \"Init loss: \"\n",
        "            + \"{:10.4f}\".format(jnp.array(losses[0]))\n",
        "            + f\"; Avg loss (last {print_steps} iter): \"\n",
        "            + \"{:10.4f}\".format(jnp.array(losses[-100:]).mean())\n",
        "        )\n",
        "\n",
        "    if (step + 1) % 2500 == 0 or step == num_steps - 1:\n",
        "        # Save intermediate results\n",
        "        estimated_params = svi_batch.get_params(svi_state)\n",
        "\n",
        "        neutral_mean = (\n",
        "            estimated_params[\"mu_beta\"] + estimated_params[\"sigma_beta\"] ** 2 / 2\n",
        "        )\n",
        "\n",
        "        positive_mean = (\n",
        "            estimated_params[\"mu_beta\"]\n",
        "            + estimated_params[\"mu_eta\"]\n",
        "            + (estimated_params[\"sigma_beta\"] ** 2 + estimated_params[\"sigma_eta\"] ** 2)\n",
        "            / 2\n",
        "        )\n",
        "\n",
        "        negative_mean = (\n",
        "            estimated_params[\"mu_beta\"]\n",
        "            - estimated_params[\"mu_eta\"]\n",
        "            + (estimated_params[\"sigma_beta\"] ** 2 + estimated_params[\"sigma_eta\"] ** 2)\n",
        "            / 2\n",
        "        )\n",
        "\n",
        "        np.save(\"neutral_topic_mean.npy\", neutral_mean)\n",
        "        np.save(\"negative_topic_mean.npy\", positive_mean)\n",
        "        np.save(\"positive_topic_mean.npy\", negative_mean)\n",
        "\n",
        "        topics = get_topics(neutral_mean, positive_mean, negative_mean, vocabulary)\n",
        "\n",
        "        with open(\"topics.txt\", \"w\") as f:\n",
        "            print(topics, file=f)\n",
        "\n",
        "        authors = pd.DataFrame(\n",
        "            {\"name\": author_map, \"ideal_point\": np.array(estimated_params[\"mu_x\"])}\n",
        "        )\n",
        "        authors.to_csv(\"authors.csv\")\n",
        "\n",
        "        if print_intermediate_results:\n",
        "            print(f\"Results after {step} steps.\")\n",
        "            print(topics)\n",
        "            sorted_authors = \"Authors sorted by their ideal points: \" + \",\".join(\n",
        "                list(authors.sort_values(\"ideal_point\")[\"name\"])\n",
        "            )\n",
        "            print(sorted_authors.replace(\"\\n\", \" \"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
