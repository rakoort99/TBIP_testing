{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert Senate speech data from 114th Congress to bag of words format.\n",
    "\n",
    "The data is provided by [1]. Specifically, we use the `hein-daily` data. To \n",
    "run this script, make sure the relevant files are in \n",
    "`data/senate-speeches-114/raw/`. The files needed for this script are \n",
    "`speeches_114.txt`, `descr_114.txt`, and `114_SpeakerMap.txt`.\n",
    "\n",
    "#### References\n",
    "[1]: Gentzkow, Matthew, Jesse M. Shapiro, and Matt Taddy. Congressional Record \n",
    "     for the 43rd-114th Congresses: Parsed Speeches and Phrase Counts. Palo \n",
    "     Alto, CA: Stanford Libraries [distributor], 2018-01-16. \n",
    "     https://data.stanford.edu/congress_text\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import setup_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkstops = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set params\n",
    "\n",
    "stops = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(\n",
    "    os.path.join(os.path.dirname('.'), os.pardir)) \n",
    "data_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\raw\")\n",
    "save_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\clean\\\\\"+stops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>it's been a number of years since i've been an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>just a brief update for the high school commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>please email me your speech documents. i have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Maggie Berthiaume</td>\n",
       "      <td>maggie berthiaume woodward academy current coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Bill Russell</td>\n",
       "      <td>bill russell judge philosophy overview- i love...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Judge Name                                           Paradigm\n",
       "Judge ID                                                                      \n",
       "3               Aaron Hardy  it's been a number of years since i've been an...\n",
       "1057          Shunta Jordan  just a brief update for the high school commun...\n",
       "1088            Bill Smelko  please email me your speech documents. i have ...\n",
       "1265      Maggie Berthiaume  maggie berthiaume woodward academy current coa...\n",
       "1464           Bill Russell  bill russell judge philosophy overview- i love..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_markdown_to_plain_text(markdown_string):\n",
    "\n",
    "    markdown_string = str(markdown_string)\n",
    "    # Remove newlines\n",
    "    plain_text = markdown_string.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove bold formatting (e.g., **text**)\n",
    "    plain_text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', plain_text)\n",
    "    \n",
    "    # Remove backslashes\n",
    "    plain_text = plain_text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove other markdown formatting if needed\n",
    "    plain_text = plain_text.replace('\\t', '')\n",
    "    \n",
    "    return plain_text\n",
    "\n",
    "def fix_spaces(string):\n",
    "    new_string = ' '.join(string.strip().split())\n",
    "    return new_string\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'paradigms.csv'), index_col=0)[['Judge Name', 'Paradigm',\"Judge's CEDA rounds\"]]\n",
    "df = df[df[\"Judge's CEDA rounds\"] > 5] #scare away tabroom ghosts\n",
    "df.drop(\"Judge's CEDA rounds\", axis=1, inplace=True)\n",
    "df['Paradigm'] = df['Paradigm'].apply(lambda x: x.encode(\"utf-8\").decode(\"utf-8\").lower())\n",
    "df['Paradigm'] = df['Paradigm'].apply(convert_markdown_to_plain_text)\n",
    "df['Paradigm'] = df['Paradigm'].apply(fix_spaces)\n",
    "df.sort_index(inplace=True)\n",
    "df = df[~df['Judge Name'].isna()]\n",
    "df = df[df['Paradigm'].str.len() > 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>[[it, 's, been, a, number, of, years, since, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>[[just, a, brief, update, for, the, high, scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>[[please, email, me, your, speech, documents, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Maggie Berthiaume</td>\n",
       "      <td>[[maggie, berthiaume, woodward, academy, curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Bill Russell</td>\n",
       "      <td>[[bill, russell, judge, philosophy, overview-,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Judge Name                                           Paradigm\n",
       "Judge ID                                                                      \n",
       "3               Aaron Hardy  [[it, 's, been, a, number, of, years, since, i...\n",
       "1057          Shunta Jordan  [[just, a, brief, update, for, the, high, scho...\n",
       "1088            Bill Smelko  [[please, email, me, your, speech, documents, ...\n",
       "1265      Maggie Berthiaume  [[maggie, berthiaume, woodward, academy, curre...\n",
       "1464           Bill Russell  [[bill, russell, judge, philosophy, overview-,..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "df['Paradigm'] = df['Paradigm'].apply(lambda x: [word_tokenize(t) for t in sent_tokenize(x)])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_raw = len(set(item for sublist in clone for item in sublist))\n",
    "length_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stupid phraseizer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_word_occurrences(documents, T):\n",
    "    delta = 2\n",
    "    word_counts = defaultdict(int)\n",
    "    word_following_counts = defaultdict(int)\n",
    "\n",
    "    print('iterating through documents!')\n",
    "    for document in tqdm(documents):\n",
    "        for sentence in document:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                word_i = sentence[i]\n",
    "                word_j = sentence[i + 1]\n",
    "\n",
    "                word_counts[word_i] += 1\n",
    "                word_counts[word_j] += 1\n",
    "\n",
    "                if word_i != word_j:\n",
    "                    word_following_counts[(word_i, word_j)] += 1\n",
    "    vocab_size = len(word_counts)\n",
    "    result_dict = {}\n",
    "    print('scoring terms')\n",
    "    for (word_i, word_j), count_ij in tqdm(word_following_counts.items()):\n",
    "        count_i = word_counts[word_i]\n",
    "        count_j = word_counts[word_j]\n",
    "        if count_i not in nltk.corpus.stopwords.words('english'):\n",
    "            score = (count_ij - delta)*vocab_size / (count_i * count_j)\n",
    "            if score > T:\n",
    "                result_dict[(word_i, word_j)] = word_i + '_' + word_j\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(document, replacement_dict):\n",
    "    new_doc = []\n",
    "    for sentence in document:\n",
    "        new_sentence = []\n",
    "        i = 0\n",
    "        while i < len(sentence) - 2:\n",
    "            if (sentence[i], sentence[i+1]) in replacement_dict:\n",
    "                new_sentence.append(replacement_dict[(sentence[i], sentence[i+1])])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        new_doc.append(new_sentence)\n",
    "    return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating through documents!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1262/1262 [00:01<00:00, 1235.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241894/241894 [00:34<00:00, 6997.98it/s]\n"
     ]
    }
   ],
   "source": [
    "two_grams = count_word_occurrences(df['Paradigm'], 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Paradigm'] = df['Paradigm'].apply(replacer, replacement_dict=two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26485"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_2grams = len(set(item for sublist in clone for item in sublist))\n",
    "length_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating through documents!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1262/1262 [00:00<00:00, 1316.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227315/227315 [00:31<00:00, 7256.13it/s]\n"
     ]
    }
   ],
   "source": [
    "three_four_grams = count_word_occurrences(df['Paradigm'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Paradigm'] = df['Paradigm'].apply(replacer, replacement_dict=three_four_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25550"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_4grams = len(set(item for sublist in clone for item in sublist))\n",
    "length_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def pos_tag_and_lemmatize(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_sentences = []\n",
    "    for sentence in sentences: \n",
    "        tagged_words = pos_tag(sentence)    # Perform POS tagging\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)) for word, pos in tagged_words]\n",
    "        tagged_sentences.append(lemmatized_words)\n",
    "    return tagged_sentences\n",
    "\n",
    "df['Paradigm'] = df['Paradigm'].apply(pos_tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22526"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_lemma = len(set(item for sublist in clone for item in sublist))\n",
    "length_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262,)\n",
      "(1262,)\n",
      "(1262,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "speaker = np.array(df.index.values)\n",
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "speeches = list(clone)\n",
    "\n",
    "# Create mapping between names and IDs.\n",
    "speaker_to_speaker_id = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speaker])\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "\n",
    "print(author_map.shape)\n",
    "print(author_indices.shape)\n",
    "print(speaker.shape)\n",
    "len(set(speaker))\n",
    "\n",
    "speeches = [' '.join(doc) for doc in speeches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_stops = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltkstops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if find_stops:\n",
    "    count_vectorizer = CountVectorizer(min_df=1,\n",
    "                                   max_df=1., \n",
    "                                   ngram_range=(1, 1),\n",
    "                                   stop_words=nltkstops)\n",
    "\n",
    "\n",
    "    # Learn initial document term matrix. This is only initial because we use it to\n",
    "    # identify words to exclude based on author counts.\n",
    "    counts = count_vectorizer.fit_transform(speeches)\n",
    "\n",
    "    vocabulary = np.array(\n",
    "        [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                                key=lambda kv: kv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15974\n",
      "(1262, 15974)\n",
      "(1262, 15974)\n",
      "(15974,)\n",
      "(15974,)\n",
      "(15974,)\n",
      "(15974,)\n",
      "(15974,)\n"
     ]
    }
   ],
   "source": [
    "counts_per_author = utils.bincount_2d(author_indices, counts.toarray())\n",
    "min_authors_per_word = 8\n",
    "author_counts_per_word = np.sum(counts_per_author > 0, axis=0)\n",
    "ranking = np.flip(np.argsort(author_counts_per_word))\n",
    "TF = counts.toarray().sum(axis=0)/counts.toarray().sum(axis=0).sum()\n",
    "\n",
    "IDF = np.log((author_counts_per_word * (1/counts.shape[0]))**-1)\n",
    "\n",
    "\n",
    "terms_per_author = counts.toarray().sum(axis=1)\n",
    "terms_per_author = np.where(terms_per_author==0, 1, terms_per_author)\n",
    "tfidfpart1 = counts.toarray()/terms_per_author[:, np.newaxis]\n",
    "\n",
    "\n",
    "TFIDF = (tfidfpart1 * np.exp(IDF)).sum(axis=0) / np.where(author_counts_per_word==0, 1, author_counts_per_word).astype(float)\n",
    "\n",
    "ppt = counts.toarray() / counts.toarray().sum(axis=0)\n",
    "entropy = (np.multiply(ppt, -np.log(np.where(ppt==0, 1, ppt)))).sum(axis=0)\n",
    "\n",
    "\n",
    "print(len(vocabulary))\n",
    "print(counts.shape)\n",
    "print(counts_per_author.shape)\n",
    "print(author_counts_per_word.shape)\n",
    "print(TF.shape)\n",
    "print(IDF.shape)\n",
    "#vocabulary[ranking][:20]\n",
    "print(TFIDF.shape)\n",
    "print(entropy.T.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>debate</td>\n",
       "      <td>0.027533</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>6.757441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>argument</td>\n",
       "      <td>0.019652</td>\n",
       "      <td>0.113139</td>\n",
       "      <td>0.026160</td>\n",
       "      <td>6.669567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14259</th>\n",
       "      <td>think</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>0.324813</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>6.401307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>make</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.229702</td>\n",
       "      <td>0.016890</td>\n",
       "      <td>6.592842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>aff</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.472225</td>\n",
       "      <td>0.022234</td>\n",
       "      <td>6.283383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5472</th>\n",
       "      <td>fathom</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>4.949020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>fatigue</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2.848758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>faultcx</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2.390152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>faux</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>4.459364</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>kathleenrock2056</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2.273874</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15974 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words        TF       IDF     TFIDF   entropy\n",
       "3789             debate  0.027533  0.055389  0.034200  6.757441\n",
       "1181           argument  0.019652  0.113139  0.026160  6.669567\n",
       "14259             think  0.011251  0.324813  0.019743  6.401307\n",
       "8713               make  0.010117  0.229702  0.016890  6.592842\n",
       "712                 aff  0.009906  0.472225  0.022234  6.283383\n",
       "...                 ...       ...       ...       ...       ...\n",
       "5472             fathom  0.000003  7.140453  4.949020  0.000000\n",
       "5473            fatigue  0.000003  7.140453  2.848758  0.000000\n",
       "5475            faultcx  0.000003  7.140453  2.390152  0.000000\n",
       "5477               faux  0.000003  7.140453  4.459364  0.000000\n",
       "7987   kathleenrock2056  0.000003  7.140453  2.273874  0.000000\n",
       "\n",
       "[15974 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = pd.DataFrame({'words':vocabulary, 'TF':TF, 'IDF':IDF, 'TFIDF':TFIDF, 'entropy':entropy})\n",
    "vocab_df.sort_values('TF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_TF = vocab_df.sort_values('TF', ascending=False)[['words','TF']].iloc[:1500].set_index('words')\n",
    "bot_IDF = vocab_df.sort_values('IDF', ascending=True)[['words','IDF']].iloc[:1500].set_index('words')\n",
    "bot_TFIDF = vocab_df.sort_values('TFIDF', ascending=True)[['words','TFIDF']].iloc[:1500].set_index('words')\n",
    "top_entropy = vocab_df.sort_values('entropy', ascending=False)[['words','entropy']].iloc[:1500].set_index('words')\n",
    "\n",
    "full = top_TF.join([bot_IDF,bot_TFIDF,top_entropy], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1653, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full.to_excel('check_for_stops.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops = pd.read_excel('check_for_stops.xlsx')\n",
    "new_stops = list(new_stops[new_stops['include']=='bad']['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops = list(set(nltkstops) | set(new_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vectorizer = CountVectorizer(min_df=5,\n",
    "                                max_df=1., \n",
    "                                ngram_range=(1, 1),\n",
    "                                stop_words=new_stops)\n",
    "\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1259 1260 1261]\n",
      "(1254, 4653)\n",
      "(1262,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4653"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speaker])\n",
    "# Remove speeches with not enough words.\n",
    "existing_speeches = np.where(np.sum(counts, axis=1) > 1)[0]\n",
    "counts_dense = counts[existing_speeches]\n",
    "author_indices = author_indices[existing_speeches]\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\austin\\\\Documents\\\\python-projs\\\\uchicago\\\\research\\\\Thesis\\\\TBIP_testing\\\\data\\\\paradigms\\\\clean\\\\custom'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts.npz\"),\n",
    "                sparse.csr_matrix(counts_dense).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices.npy\"), author_indices)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary.txt\"), vocabulary, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map.txt\"), author_map, fmt=\"%s\")\n",
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for document in np.array(speeches)[existing_speeches]]\n",
    "np.savetxt(os.path.join(save_dir, \"raw_documents.txt\"), \n",
    "           raw_documents, \n",
    "           fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1262"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1254"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_speeches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#og dataframe\n",
    "df.to_csv(os.path.join(save_dir, 'id_name_para.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
