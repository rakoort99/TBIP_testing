{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert Senate speech data from 114th Congress to bag of words format.\n",
    "\n",
    "The data is provided by [1]. Specifically, we use the `hein-daily` data. To \n",
    "run this script, make sure the relevant files are in \n",
    "`data/senate-speeches-114/raw/`. The files needed for this script are \n",
    "`speeches_114.txt`, `descr_114.txt`, and `114_SpeakerMap.txt`.\n",
    "\n",
    "#### References\n",
    "[1]: Gentzkow, Matthew, Jesse M. Shapiro, and Matt Taddy. Congressional Record \n",
    "     for the 43rd-114th Congresses: Parsed Speeches and Phrase Counts. Palo \n",
    "     Alto, CA: Stanford Libraries [distributor], 2018-01-16. \n",
    "     https://data.stanford.edu/congress_text\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import setup_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkstops = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set params\n",
    "\n",
    "stops = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(\n",
    "    os.path.join(os.path.dirname('.'), os.pardir)) \n",
    "data_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\raw\")\n",
    "save_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\clean\\\\\"+stops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>it's been a number of years since i've been an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>just a brief update for the high school commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>please email me your speech documents. i have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Maggie Berthiaume</td>\n",
       "      <td>maggie berthiaume woodward academy current coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Bill Russell</td>\n",
       "      <td>bill russell judge philosophy overview- i love...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Judge Name                                           Paradigm\n",
       "Judge ID                                                                      \n",
       "3               Aaron Hardy  it's been a number of years since i've been an...\n",
       "1057          Shunta Jordan  just a brief update for the high school commun...\n",
       "1088            Bill Smelko  please email me your speech documents. i have ...\n",
       "1265      Maggie Berthiaume  maggie berthiaume woodward academy current coa...\n",
       "1464           Bill Russell  bill russell judge philosophy overview- i love..."
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_markdown_to_plain_text(markdown_string):\n",
    "\n",
    "    markdown_string = str(markdown_string)\n",
    "    # Remove newlines\n",
    "    plain_text = markdown_string.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove bold formatting (e.g., **text**)\n",
    "    plain_text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', plain_text)\n",
    "    \n",
    "    # Remove backslashes\n",
    "    plain_text = plain_text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove other markdown formatting if needed\n",
    "    plain_text = plain_text.replace('\\t', '')\n",
    "    \n",
    "    return plain_text\n",
    "\n",
    "def fix_spaces(string):\n",
    "    new_string = ' '.join(string.strip().split())\n",
    "    return new_string\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'paradigms.csv'), index_col=0)[['Judge Name', 'Paradigm',\"Judge's CEDA rounds\"]]\n",
    "df = df[df[\"Judge's CEDA rounds\"] > 5] #scare away tabroom ghosts\n",
    "df.drop(\"Judge's CEDA rounds\", axis=1, inplace=True)\n",
    "df['Paradigm'] = df['Paradigm'].apply(lambda x: x.encode(\"utf-8\").decode(\"utf-8\").lower())\n",
    "df['Paradigm'] = df['Paradigm'].apply(convert_markdown_to_plain_text)\n",
    "df['Paradigm'] = df['Paradigm'].apply(fix_spaces)\n",
    "df.sort_index(inplace=True)\n",
    "df = df[~df['Judge Name'].isna()]\n",
    "df = df[df['Paradigm'].str.len() > 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>[[it, 's, been, a, number, of, years, since, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>[[just, a, brief, update, for, the, high, scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>[[please, email, me, your, speech, documents, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Maggie Berthiaume</td>\n",
       "      <td>[[maggie, berthiaume, woodward, academy, curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Bill Russell</td>\n",
       "      <td>[[bill, russell, judge, philosophy, overview-,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Judge Name                                           Paradigm\n",
       "Judge ID                                                                      \n",
       "3               Aaron Hardy  [[it, 's, been, a, number, of, years, since, i...\n",
       "1057          Shunta Jordan  [[just, a, brief, update, for, the, high, scho...\n",
       "1088            Bill Smelko  [[please, email, me, your, speech, documents, ...\n",
       "1265      Maggie Berthiaume  [[maggie, berthiaume, woodward, academy, curre...\n",
       "1464           Bill Russell  [[bill, russell, judge, philosophy, overview-,..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "df['Paradigm'] = df['Paradigm'].apply(lambda x: [word_tokenize(t) for t in sent_tokenize(x)])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26987"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_raw = len(set(item for sublist in clone for item in sublist))\n",
    "length_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stupid phraseizer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_word_occurrences(documents, T):\n",
    "    delta = 2\n",
    "    word_counts = defaultdict(int)\n",
    "    word_following_counts = defaultdict(int)\n",
    "\n",
    "    print('iterating through documents!')\n",
    "    for document in tqdm(documents):\n",
    "        for sentence in document:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                word_i = sentence[i]\n",
    "                word_j = sentence[i + 1]\n",
    "\n",
    "                word_counts[word_i] += 1\n",
    "                word_counts[word_j] += 1\n",
    "\n",
    "                if word_i != word_j:\n",
    "                    word_following_counts[(word_i, word_j)] += 1\n",
    "    vocab_size = len(word_counts)\n",
    "    result_dict = {}\n",
    "    print('scoring terms')\n",
    "    for (word_i, word_j), count_ij in tqdm(word_following_counts.items()):\n",
    "        count_i = word_counts[word_i]\n",
    "        count_j = word_counts[word_j]\n",
    "        if count_i not in nltk.corpus.stopwords.words('english'):\n",
    "            score = (count_ij - delta)*vocab_size / (count_i * count_j)\n",
    "            if score > T:\n",
    "                result_dict[(word_i, word_j)] = word_i + '_' + word_j\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(document, replacement_dict):\n",
    "    new_doc = []\n",
    "    for sentence in document:\n",
    "        new_sentence = []\n",
    "        i = 0\n",
    "        while i < len(sentence) - 2:\n",
    "            if (sentence[i], sentence[i+1]) in replacement_dict:\n",
    "                new_sentence.append(replacement_dict[(sentence[i], sentence[i+1])])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        new_doc.append(new_sentence)\n",
    "    return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating through documents!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1262/1262 [00:00<00:00, 1586.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241894/241894 [00:30<00:00, 8049.41it/s]\n"
     ]
    }
   ],
   "source": [
    "two_grams = count_word_occurrences(df['Paradigm'], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Paradigm'] = df['Paradigm'].apply(replacer, replacement_dict=two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26393"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_2grams = len(set(item for sublist in clone for item in sublist))\n",
    "length_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating through documents!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1262/1262 [00:00<00:00, 1697.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226788/226788 [00:29<00:00, 7761.65it/s]\n"
     ]
    }
   ],
   "source": [
    "three_four_grams = count_word_occurrences(df['Paradigm'], 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Paradigm'] = df['Paradigm'].apply(replacer, replacement_dict=three_four_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25356"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_4grams = len(set(item for sublist in clone for item in sublist))\n",
    "length_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def pos_tag_and_lemmatize(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_sentences = []\n",
    "    for sentence in sentences: \n",
    "        tagged_words = pos_tag(sentence)    # Perform POS tagging\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)) for word, pos in tagged_words]\n",
    "        tagged_sentences.append(lemmatized_words)\n",
    "    return tagged_sentences\n",
    "\n",
    "df['Paradigm'] = df['Paradigm'].apply(pos_tag_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22331"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "length_lemma = len(set(item for sublist in clone for item in sublist))\n",
    "length_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262,)\n",
      "(1262,)\n",
      "(1262,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "speaker = np.array(df.index.values)\n",
    "clone = df['Paradigm'].apply(lambda x: [word for sentence in x for word in sentence])\n",
    "speeches = list(clone)\n",
    "\n",
    "# Create mapping between names and IDs.\n",
    "speaker_to_speaker_id = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speaker])\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "\n",
    "print(author_map.shape)\n",
    "print(author_indices.shape)\n",
    "print(speaker.shape)\n",
    "len(set(speaker))\n",
    "\n",
    "speeches = [' '.join(doc) for doc in speeches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_stops = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltkstops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "if find_stops:\n",
    "    count_vectorizer = CountVectorizer(min_df=1,\n",
    "                                   max_df=1., \n",
    "                                   ngram_range=(1, 1),\n",
    "                                   stop_words=nltkstops)\n",
    "\n",
    "\n",
    "    # Learn initial document term matrix. This is only initial because we use it to\n",
    "    # identify words to exclude based on author counts.\n",
    "    counts = count_vectorizer.fit_transform(speeches)\n",
    "\n",
    "    vocabulary = np.array(\n",
    "        [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                                key=lambda kv: kv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15777\n",
      "(1262, 15777)\n",
      "(1262, 15777)\n",
      "(15777,)\n",
      "(15777,)\n",
      "(15777,)\n",
      "(15777,)\n",
      "(15777,)\n"
     ]
    }
   ],
   "source": [
    "counts_per_author = utils.bincount_2d(author_indices, counts.toarray())\n",
    "min_authors_per_word = 8\n",
    "author_counts_per_word = np.sum(counts_per_author > 0, axis=0)\n",
    "ranking = np.flip(np.argsort(author_counts_per_word))\n",
    "TF = counts.toarray().sum(axis=0)/counts.toarray().sum(axis=0).sum()\n",
    "\n",
    "IDF = np.log((author_counts_per_word * (1/counts.shape[0]))**-1)\n",
    "\n",
    "\n",
    "terms_per_author = counts.toarray().sum(axis=1)\n",
    "terms_per_author = np.where(terms_per_author==0, 1, terms_per_author)\n",
    "tfidfpart1 = counts.toarray()/terms_per_author[:, np.newaxis]\n",
    "\n",
    "\n",
    "TFIDF = (tfidfpart1 * np.exp(IDF)).sum(axis=0) / np.where(author_counts_per_word==0, 1, author_counts_per_word).astype(float)\n",
    "\n",
    "ppt = counts.toarray() / counts.toarray().sum(axis=0)\n",
    "entropy = (np.multiply(ppt, -np.log(np.where(ppt==0, 1, ppt)))).sum(axis=0)\n",
    "\n",
    "\n",
    "print(len(vocabulary))\n",
    "print(counts.shape)\n",
    "print(counts_per_author.shape)\n",
    "print(author_counts_per_word.shape)\n",
    "print(TF.shape)\n",
    "print(IDF.shape)\n",
    "#vocabulary[ranking][:20]\n",
    "print(TFIDF.shape)\n",
    "print(entropy.T.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>debate</td>\n",
       "      <td>0.027423</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>0.034059</td>\n",
       "      <td>6.757688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>argument</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>0.113139</td>\n",
       "      <td>0.026055</td>\n",
       "      <td>6.669567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14082</th>\n",
       "      <td>think</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.324813</td>\n",
       "      <td>0.019664</td>\n",
       "      <td>6.401307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8602</th>\n",
       "      <td>make</td>\n",
       "      <td>0.010075</td>\n",
       "      <td>0.229702</td>\n",
       "      <td>0.016818</td>\n",
       "      <td>6.592842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>aff</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.472225</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>6.283151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>familiarty</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2.253571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>famously</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>1.628387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>fanaticism</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>0.632899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>fantasize</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2.640167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>kathleenrock2056</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2.257603</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15777 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words        TF       IDF     TFIDF   entropy\n",
       "3732             debate  0.027423  0.055389  0.034059  6.757688\n",
       "1162           argument  0.019571  0.113139  0.026055  6.669567\n",
       "14082             think  0.011204  0.324813  0.019664  6.401307\n",
       "8602               make  0.010075  0.229702  0.016818  6.592842\n",
       "694                 aff  0.009867  0.472225  0.022133  6.283151\n",
       "...                 ...       ...       ...       ...       ...\n",
       "5369         familiarty  0.000003  7.140453  2.253571  0.000000\n",
       "5372           famously  0.000003  7.140453  1.628387  0.000000\n",
       "5374         fanaticism  0.000003  7.140453  0.632899  0.000000\n",
       "5377          fantasize  0.000003  7.140453  2.640167  0.000000\n",
       "7888   kathleenrock2056  0.000003  7.140453  2.257603  0.000000\n",
       "\n",
       "[15777 rows x 5 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = pd.DataFrame({'words':vocabulary, 'TF':TF, 'IDF':IDF, 'TFIDF':TFIDF, 'entropy':entropy})\n",
    "vocab_df.sort_values('TF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_TF = vocab_df.sort_values('TF', ascending=False)[['words','TF']].iloc[:1500].set_index('words')\n",
    "bot_IDF = vocab_df.sort_values('IDF', ascending=True)[['words','IDF']].iloc[:1500].set_index('words')\n",
    "bot_TFIDF = vocab_df.sort_values('TFIDF', ascending=True)[['words','TFIDF']].iloc[:1500].set_index('words')\n",
    "top_entropy = vocab_df.sort_values('entropy', ascending=False)[['words','entropy']].iloc[:1500].set_index('words')\n",
    "\n",
    "full = top_TF.join([bot_IDF,bot_TFIDF,top_entropy], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1650, 4)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'check_for_stops.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m full\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39mcheck_for_stops.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\core\\generic.py:2374\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2361\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mformats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexcel\u001b[39;00m \u001b[39mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2363\u001b[0m formatter \u001b[39m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2364\u001b[0m     df,\n\u001b[0;32m   2365\u001b[0m     na_rep\u001b[39m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     inf_rep\u001b[39m=\u001b[39minf_rep,\n\u001b[0;32m   2373\u001b[0m )\n\u001b[1;32m-> 2374\u001b[0m formatter\u001b[39m.\u001b[39mwrite(\n\u001b[0;32m   2375\u001b[0m     excel_writer,\n\u001b[0;32m   2376\u001b[0m     sheet_name\u001b[39m=\u001b[39msheet_name,\n\u001b[0;32m   2377\u001b[0m     startrow\u001b[39m=\u001b[39mstartrow,\n\u001b[0;32m   2378\u001b[0m     startcol\u001b[39m=\u001b[39mstartcol,\n\u001b[0;32m   2379\u001b[0m     freeze_panes\u001b[39m=\u001b[39mfreeze_panes,\n\u001b[0;32m   2380\u001b[0m     engine\u001b[39m=\u001b[39mengine,\n\u001b[0;32m   2381\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   2382\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:944\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    940\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     \u001b[39m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[0;32m    943\u001b[0m     \u001b[39m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[1;32m--> 944\u001b[0m     writer \u001b[39m=\u001b[39m ExcelWriter(  \u001b[39m# type: ignore[abstract]\u001b[39;00m\n\u001b[0;32m    945\u001b[0m         writer, engine\u001b[39m=\u001b[39mengine, storage_options\u001b[39m=\u001b[39mstorage_options\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:60\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenpyxl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkbook\u001b[39;00m \u001b[39mimport\u001b[39;00m Workbook\n\u001b[0;32m     58\u001b[0m engine_kwargs \u001b[39m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 60\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     61\u001b[0m     path,\n\u001b[0;32m     62\u001b[0m     mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m     63\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m     64\u001b[0m     if_sheet_exists\u001b[39m=\u001b[39mif_sheet_exists,\n\u001b[0;32m     65\u001b[0m     engine_kwargs\u001b[39m=\u001b[39mengine_kwargs,\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[39m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode:  \u001b[39m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1313\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handles \u001b[39m=\u001b[39m IOHandles(\n\u001b[0;32m   1310\u001b[0m     cast(IO[\u001b[39mbytes\u001b[39m], path), compression\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m   1311\u001b[0m )\n\u001b[0;32m   1312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1314\u001b[0m         path, mode, storage_options\u001b[39m=\u001b[39mstorage_options, is_text\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     )\n\u001b[0;32m   1316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cur_sheet \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m date_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\austin\\miniconda3\\envs\\thesis_env\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'check_for_stops.xlsx'"
     ]
    }
   ],
   "source": [
    "# full.to_excel('check_for_stops.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops = pd.read_excel('check_for_stops.xlsx')\n",
    "new_stops = list(new_stops[new_stops['include']=='bad']['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops = list(set(nltkstops) | set(new_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1,\n",
    "                                max_df=1., \n",
    "                                ngram_range=(1, 1),\n",
    "                                stop_words=new_stops)\n",
    "\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15748"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove speeches with not enough words.\n",
    "existing_speeches = np.where(np.sum(counts, axis=1) > 1)[0]\n",
    "counts_dense = counts[existing_speeches]\n",
    "author_indices = author_indices[existing_speeches]\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\austin\\\\Documents\\\\python-projs\\\\uchicago\\\\research\\\\Thesis\\\\TBIP_testing\\\\data\\\\paradigms\\\\clean\\\\custom'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts.npz\"),\n",
    "                sparse.csr_matrix(counts_dense).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices.npy\"), author_indices)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary.txt\"), vocabulary, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map.txt\"), author_map, fmt=\"%s\")\n",
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for document in np.array(speeches)[existing_speeches]]\n",
    "np.savetxt(os.path.join(save_dir, \"raw_documents.txt\"), \n",
    "           raw_documents, \n",
    "           fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1262"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1255"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_speeches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#og dataframe\n",
    "df.to_csv(os.path.join(save_dir, 'id_name_para.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
