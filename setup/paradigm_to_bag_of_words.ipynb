{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert Senate speech data from 114th Congress to bag of words format.\n",
    "\n",
    "The data is provided by [1]. Specifically, we use the `hein-daily` data. To \n",
    "run this script, make sure the relevant files are in \n",
    "`data/senate-speeches-114/raw/`. The files needed for this script are \n",
    "`speeches_114.txt`, `descr_114.txt`, and `114_SpeakerMap.txt`.\n",
    "\n",
    "#### References\n",
    "[1]: Gentzkow, Matthew, Jesse M. Shapiro, and Matt Taddy. Congressional Record \n",
    "     for the 43rd-114th Congresses: Parsed Speeches and Phrase Counts. Palo \n",
    "     Alto, CA: Stanford Libraries [distributor], 2018-01-16. \n",
    "     https://data.stanford.edu/congress_text\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import setup_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(\n",
    "    os.path.join(os.path.dirname('.'), os.pardir)) \n",
    "data_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\raw\")\n",
    "save_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\clean\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris Palmer</td>\n",
       "      <td>Tabroom.com is mostly my fault. Therefore I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>It's been a number of years since I've been an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Paul Wexler</td>\n",
       "      <td>Debate Paradigm Paul Wexler Coach since 1993, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>Just a brief update for the high school commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>Please email me your speech documents. I have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Judge Name                                           Paradigm\n",
       "Judge ID                                                                  \n",
       "1          Chris Palmer  Tabroom.com is mostly my fault. Therefore I'm ...\n",
       "3           Aaron Hardy  It's been a number of years since I've been an...\n",
       "319         Paul Wexler  Debate Paradigm Paul Wexler Coach since 1993, ...\n",
       "1057      Shunta Jordan  Just a brief update for the high school commun...\n",
       "1088        Bill Smelko  Please email me your speech documents. I have ..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_markdown_to_plain_text(markdown_string):\n",
    "\n",
    "    markdown_string = str(markdown_string)\n",
    "    # Remove newlines\n",
    "    plain_text = markdown_string.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove bold formatting (e.g., **text**)\n",
    "    plain_text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', plain_text)\n",
    "    \n",
    "    # Remove backslashes\n",
    "    plain_text = plain_text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove other markdown formatting if needed\n",
    "    plain_text = plain_text.replace('\\t', '')\n",
    "    \n",
    "    return plain_text\n",
    "\n",
    "def fix_spaces(string):\n",
    "    new_string = ' '.join(string.strip().split())\n",
    "    return new_string\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'paradigms.csv'), index_col=0)[['Judge Name', 'Paradigm']]\n",
    "\n",
    "df['Paradigm'] = df['Paradigm'].apply(convert_markdown_to_plain_text)\n",
    "df['Paradigm'] = df['Paradigm'].apply(fix_spaces)\n",
    "df.sort_index(inplace=True)\n",
    "df = df[~df['Judge Name'].isna()]\n",
    "df = df[df['Paradigm'].str.len() > 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1817,)\n",
      "(1817,)\n",
      "(1817,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1817"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "speaker = np.array(df.index.values)\n",
    "\n",
    "speeches = np.array(df['Paradigm'])\n",
    "\n",
    "\n",
    "# Create mapping between names and IDs.\n",
    "speaker_to_speaker_id = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speaker])\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "\n",
    "print(author_map.shape)\n",
    "print(author_indices.shape)\n",
    "print(speaker.shape)\n",
    "len(set(speaker))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'you',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'for',\n",
       " 'it',\n",
       " 'debate',\n",
       " 'if',\n",
       " 'not',\n",
       " 'be',\n",
       " 'are',\n",
       " 'on',\n",
       " 'your',\n",
       " 'have',\n",
       " 'me',\n",
       " 'or',\n",
       " 'as',\n",
       " 'arguments',\n",
       " 'will',\n",
       " 'but',\n",
       " 'my',\n",
       " 's',\n",
       " 'with',\n",
       " 'this',\n",
       " 'do',\n",
       " 'of the',\n",
       " 'should',\n",
       " 'if you',\n",
       " 'don',\n",
       " 'don t',\n",
       " 'what',\n",
       " 'an',\n",
       " 'can',\n",
       " 'think',\n",
       " 'in the',\n",
       " 'more',\n",
       " 'am',\n",
       " 'argument',\n",
       " 'at',\n",
       " 'about',\n",
       " 'i am',\n",
       " 'i will',\n",
       " 'debates',\n",
       " 'so',\n",
       " 'round',\n",
       " 'make',\n",
       " 'm',\n",
       " 'i m',\n",
       " 'they',\n",
       " 'just',\n",
       " 'vote',\n",
       " 'how',\n",
       " 'than',\n",
       " 'other',\n",
       " 'to the',\n",
       " 'the debate',\n",
       " 'why',\n",
       " 'by',\n",
       " 'when',\n",
       " 'to be',\n",
       " 'read',\n",
       " 'i think',\n",
       " 'all',\n",
       " 'team',\n",
       " 'judge',\n",
       " 'most',\n",
       " 'some',\n",
       " 'i have',\n",
       " 'on the',\n",
       " 'there',\n",
       " 'their',\n",
       " 'them',\n",
       " 'also',\n",
       " 'is a',\n",
       " 'from',\n",
       " 'out',\n",
       " 'these',\n",
       " 'you are',\n",
       " 'any',\n",
       " 'please',\n",
       " 'well',\n",
       " 'because',\n",
       " 'that i',\n",
       " 're',\n",
       " 'it is',\n",
       " 'need',\n",
       " 'i don',\n",
       " 'i don t',\n",
       " 'one',\n",
       " 'it s',\n",
       " 'no',\n",
       " 'very',\n",
       " 'important',\n",
       " 'for the']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "stopwords = list(\n",
    "    np.loadtxt('stops.txt',\n",
    "               dtype=str,\n",
    "               delimiter=\",\")[0:100,0])\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=0.001,\n",
    "                                   max_df=0.3, \n",
    "                                   stop_words='english', \n",
    "                                   ngram_range=(1, 3),\n",
    "                                   token_pattern=\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "# Learn initial document term matrix. This is only initial because we use it to\n",
    "# identify words to exclude based on author counts.\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'jargon topic', 'jargon understand', 't begin',\n",
       "       'jargon used', 't based', 'jargon want', 't ballot unless',\n",
       "       't ballot clear', 't bad thing', 'jason regnier', 'jazmine',\n",
       "       't awesome', 'jccc climate', 'jeanette', 'jeanette rodriguez',\n",
       "       'jeanette rodriguez toni', 't automatically presume',\n",
       "       'jerk format evidence', 'jerk format', 't authors', 'jerk debate',\n",
       "       't automatically assume', 'jeremy hammond', 'jargon think',\n",
       "       'jeopardize', 'jefferson high school', 'jefferson high',\n",
       "       'jefferey yan', 'jefferey', 'jeff buntin northwestern',\n",
       "       't automatically mean', 'jenks', 'jargon theory',\n",
       "       't believe judge', 'jargon team', 'jargon concepts',\n",
       "       'jargon clearly impacted', 'jargon believe', 'jargon b',\n",
       "       't believe m', 'jargon argument', 'jargon don', 't believe need',\n",
       "       'jared anderson', 't believe teams', 't belittle opponents',\n",
       "       'jan feb topic', 'jan feb', 'james madison years', 't believe s',\n",
       "       'jerk impact', 'jargon don t', 'jargon explain means', 'jargon t',\n",
       "       'jargon stuff', 'jargon shorthand', 'jargon said', 'jargon s',\n",
       "       'jargon policy', 't believe kritik', 'jargon neg',\n",
       "       'jargon laden number', 'jargon laden', 'jargon know',\n",
       "       'jargon k err', 'jargon having', 't believe just', 'jargon makes',\n",
       "       'jerk just', 'jerk lower', 'jerk non', 't articulate link',\n",
       "       't articulate reason', 't ask aff', 'job disclosing arguments',\n",
       "       'job disclosing', 'job demonstrating effect', 't article',\n",
       "       't ask answer', 'job debating matter', 't ask personally',\n",
       "       'job debaters explain', 't ask questions', 't asking team',\n",
       "       'job debate', 'job debating uniqueness', 'job comparisons make',\n",
       "       'job explain evidence', 'job explaining argument', 'job kritiks',\n",
       "       'job justifying', 'job judge debate', 't arguments neg',\n",
       "       'job impact framing', 't arguments s', 'job explain significance',\n",
       "       'job harder', 'job framing debate', 't arguments unless',\n",
       "       'job frame', 'job extending', 'job explaining simply',\n",
       "       'job explaining arguments', 'job given', 't belong debate',\n",
       "       'job comparisons', 'job comparing', 't assume saying',\n",
       "       'jesus christ', 't assume understand', 't assuming', 'jesse',\n",
       "       'jersey urban debate', 't assume means', 'jersey urban',\n",
       "       'jerks love', 'jerks like speaker', 'jerks debates', 'jerk team',\n",
       "       'jerk s', 'jerk really', 'jerks love comedy',\n",
       "       'job comparing claims', 'jim lyle', 't assume flow',\n",
       "       'job communicating', 'job clearly', 'job clear say', 'job clear',\n",
       "       'job articulate', 'job answering', 'jist', 't assertive',\n",
       "       'jo debater', 'jo', 't asshole', 'jmu debater years',\n",
       "       'jmu debater', 'jmu debate team', 'jo debater year', 'job look',\n",
       "       't best friends', 'jake lee', 'issues importance',\n",
       "       'issues importance debate', 't care stand',\n",
       "       'issues important think', 'issues interact', 'issues involving',\n",
       "       't care sit', 'issues just don', 't care policy',\n",
       "       'issues like conditionality', 'issues likely team',\n",
       "       'issues literature', 'issues long', 't care make', 't care look',\n",
       "       'issues makes', 'issues mind', 'issues politics',\n",
       "       'issues policymaker', 'issues policy maker',\n",
       "       'issues policy debate', 't care going', 'issues play',\n",
       "       'issues implicate impacts', 'issues paramount', 'issues offense',\n",
       "       'issues note', 'issues negative', 'issues neg', 't care impact',\n",
       "       'issues necessary', 'issues offense defense', 'issues implicate',\n",
       "       'issues haven t', 'issues haven', 'issues explained',\n",
       "       'issues exceptions', 'issues evaluate', 'issues ethics',\n",
       "       'issues enjoy', 'issues end debate', 'issues fact', 't care type',\n",
       "       't care want', 't care way', 'issues discussing', 't career',\n",
       "       'issues difficult', 'issues depth', 'issues e', 'issues present',\n",
       "       'issues far', 'issues feel free', 'issues happen unnecessary',\n",
       "       'issues happen', 'issues hand', 'issues going',\n",
       "       'issues framework does', 't care t', 'issues far beneficial',\n",
       "       'issues frames', 'issues fix', 'issues firmly believe',\n",
       "       'issues firmly', 'issues finding flows', 'issues finding',\n",
       "       'issues feel like', 'issues flowing', 'issues presume',\n",
       "       'issues presume theory', 'issues pretty', 'iterates', 'items make',\n",
       "       't brush', 'issuing challenge', 'issuing', 'issues yes win',\n",
       "       't brightline just', 'issues worked issue', 'issues work hard',\n",
       "       't bully', 't butthole', 't c', 't calling', 'issues unless',\n",
       "       'issues worked', 'issues understand', 't brightline',\n",
       "       'iterative testing persuasive', 'jacques derrida', 'jacob bosley',\n",
       "       't biases affect', 't bigot', 'jacked', 'jack howe', 't break',\n",
       "       't black', 'j d university', 't blatantly', 't block',\n",
       "       'ive judged', 't bother m', 't bother topicality', 'j v',\n",
       "       't bias theory', 'issues trying', 'issues topicality ll',\n",
       "       'issues saturdays college', 'issues saturdays',\n",
       "       'issues said going', 'issues s', 'issues respectful',\n",
       "       'issues resolve', 'issues section', 'issues related topic',\n",
       "       'issues regarding', 'issues reasons', 'issues raised',\n",
       "       'issues race class', 'issues questions', 'issues prioritized',\n",
       "       't care das', 'issues try', 'issues seriously',\n",
       "       'issues speaker points', 't canceled', 'issues time',\n",
       "       'issues things', 'issues thing', 't canceled opportunity',\n",
       "       'issues theoretical', 'issues speaker', 'issues tend think',\n",
       "       'issues team', 'issues t', 'issues suggest', 'issues stock issues',\n",
       "       'issues stock', 't cap k', 'issues tend default',\n",
       "       't arguments make', 't arguments k', 't arguments good',\n",
       "       't accept', 'judge educational', 'judge encourage', 't abusive',\n",
       "       'judge endorse', 'judge enjoy', 'judge ethical', 't absurd',\n",
       "       'judge evaluate debate', 't able write',\n",
       "       'judge evaluating evidence', 'judge event', 'judge events',\n",
       "       't able run', 'judge example', 'judge exception',\n",
       "       'judge extremely annoyed', 'judge framework based',\n",
       "       'systemic impact', 'judge foremost', 'judge follows',\n",
       "       'judge follow speech', 'judge follow', 'judge doing different',\n",
       "       't abandon', 'judge feel like', 't able catch', 'judge familiar',\n",
       "       't able judge', 'judge facilitate exchange', 'judge facilitate',\n",
       "       'judge figure', 't action', 'judge district', 'judge disclosure',\n",
       "       't advance theory', 'judge debate way', 't aff s',\n",
       "       'judge debate tell', 'judge debate space', 'judge debate round',\n",
       "       't adequate', 'judge debate regardless', 'judge debate like',\n",
       "       'judge debate game', 'judge debate flow', 'judge debate don',\n",
       "       't aff teams', 't affect decision', 'judge debate policy',\n",
       "       'judge framework debate', 't addressed', 'judge debates debate',\n",
       "       'judge disads', 'judge differently', 't activity', 't actual',\n",
       "       'judge default', 'judge decision making', 't add',\n",
       "       't actually know', 'judge decided neg', 'judge decide debate',\n",
       "       't adapt just', 'judge debates way', 'judge debates try',\n",
       "       'judge debates high', 'judge deciding', 'judge framing',\n",
       "       'judge frequently', 'syracuse university',\n",
       "       'sympathetic negative presumption',\n",
       "       'sympathetic negative topicality', 'judge kick condo',\n",
       "       'judge kick conditional', 'judge kick choice', 'judge kick bad',\n",
       "       'judge kick early', 'sympathetic reasonability',\n",
       "       'judge kick advocacies', 'judge k debates', 'judge k debate',\n",
       "       'sympathetic theoretical', 'judge just like', 'judge just kick',\n",
       "       'judge kick alt', 'sympathetic theoretical objections',\n",
       "       'judge kick including', 'judge kick m', 'judge kinda',\n",
       "       'judge kind', 'judge kicks', 'judge kicking cps',\n",
       "       'judge kicking counterplan', 'judge kicking bad',\n",
       "       'judge kick judge', 'sympathetic kinds',\n",
       "       'sympathetic limits arguments', 'judge kick tend',\n",
       "       'judge kick stupid', 'sympathetic negative claims',\n",
       "       'judge kick neg', 'judge kick make', 'sympathetic limits',\n",
       "       'judge cut', 'judge judging round', 'judge judge lot',\n",
       "       'synthesis good', 'judge hostile approaches', 'judge hostile',\n",
       "       'judge high theory', 'synthesizes key', 'judge heavily',\n",
       "       'judge impact calculus', 'judge happy', 'judge guess',\n",
       "       'synthesizes key issues', 'syracuse', 'judge given', 'judge gets',\n",
       "       'judge generic', 'judge handful', 'sympathetic theory',\n",
       "       'judge impacts', 'syntax', 'judge job', 'judge jk',\n",
       "       'judge involved', 'judge intervention worst',\n",
       "       'judge intervention round', 'judge intervention prefer',\n",
       "       'judge important treat', 'judge intervention make',\n",
       "       'judge intervention feel', 'judge instruction telling',\n",
       "       'judge instruction prioritize', 'judge instruction important',\n",
       "       'judge instruction impact', 'judge instruction arguments',\n",
       "       'sympathy ar', 'judge cross', 'judge critiques', 'judge critique',\n",
       "       't answer team', 'jordan ut college', 'jordan ut', 't anticipate',\n",
       "       'jones says', 'jones ev', 't answer question', 't apologize',\n",
       "       'jonathan shane', 't applicable', 't applies', 'jokes people',\n",
       "       'jokes good', 'jokes appreciated', 'jonathan shane head',\n",
       "       't appreciate aff', 'joshua f', 'jot', 't answer case',\n",
       "       'jshane fsu edu', 'jshane fsu', 'jshane', 'joy watch',\n",
       "       't answer don', 'joshua f johnwell', 'journey journey helped',\n",
       "       'journey helped signpost', 'journey helped', 't answer link',\n",
       "       't answer m', 'journalism', 't answer politics', 'journey journey',\n",
       "       't answer arguments', 'joke ll', 't appreciate debaters',\n",
       "       't argument make', 'job simply vote', 'job simply',\n",
       "       'job showing relate', 'job showing', 'job seriously help',\n",
       "       'job tell evaluate', 't argument nc', 'job s', 'job responding',\n",
       "       'job playing defense', 'job playing', 'job persuade',\n",
       "       'job make sure', 't argument nr', 'joke fun', 'job use',\n",
       "       't arguing', 'joins', 'joining', 'joined activity',\n",
       "       't appreciated', 't appropriate', 'johnwell queer',\n",
       "       'job use advantage', 'johnwell', 't appropriate theory',\n",
       "       't ar says', 'john yoo', 't arbitrary self', 't aren', 't aren t',\n",
       "       'johnson evidence', 't case list', 't answer ac', 'judge accept',\n",
       "       'judge college debate', 't afraid defend', 'judge coach just',\n",
       "       't afraid ll', 'judge classroom', 'judge clash civilizations',\n",
       "       'judge college policy', 't afraid point', 'judge choice argument',\n",
       "       't afraid read', 'judge certainly don', 't afraid risks',\n",
       "       'judge caveat', 't afraid say', 'judge clarity', 'judge cards',\n",
       "       'judge comes', 'judge conditional', 'judge critic argument',\n",
       "       't affirm resolution', 'judge cp da', 'judge cover',\n",
       "       'judge couple', 't affirmative random', 'judge compare',\n",
       "       'judge counter', 'judge consider counterplan', 't affs plans',\n",
       "       'judge conditionality judge', 'judge conditionality bad',\n",
       "       't afraid ask', 'judge conditional implies',\n",
       "       'judge contextualized', 'judge ability kick', 'judge break',\n",
       "       'judge blank', 'judge alot', 'judge affs don', 'judge affs',\n",
       "       't allow affirmative', 'judge affirmative negative',\n",
       "       'judge aff really', 'judge approach', 't allowed interpretation',\n",
       "       'judge addition', 't allowed nr', 't analysis', 'judge account',\n",
       "       't annoying', 'judge according', 'judge additional',\n",
       "       'judge blank slate', 't alarmed', 'judge arguments aff',\n",
       "       'judge biased perspective', 'judge biased', 'judge bias',\n",
       "       'judge baudrillard', 'judge based said', 't afraid use',\n",
       "       't agree think', 'judge base', 't agent', 'judge attempt',\n",
       "       'judge assign', 'judge asks', 'judge ask',\n",
       "       'judge arguments debate', 't afraid vote', 'judge kinds',\n",
       "       'issues debaters want', 'issues debate think',\n",
       "       'intrinsicness theory', 't defense abusive', 'introduce round',\n",
       "       'introduce theory', 'introduce theory explain', 't defended',\n",
       "       't defend usfg', 'introduced framework', 'introduced framework k',\n",
       "       'introduced nc', 'introduced nc order', 'introduced probably',\n",
       "       't defend things', 't defend normative', 't defend issues',\n",
       "       'introduction arguments', 'introduction debated',\n",
       "       'intuitive inverse ks', 'intuitive inverse', 't defend aff',\n",
       "       'intuitive argument', 'intuitive advantage cp',\n",
       "       't defend consequences', 'intrinsicness permutations',\n",
       "       'intuit content args', 't defend instrumental', 'introspection',\n",
       "       'introductions', 'introduction proactively fact',\n",
       "       'introduction proactively', 'introduction judging',\n",
       "       'intuit content', 'intrinsicness perm', 'intrinsicness das',\n",
       "       'intrinsicness arguments uphill', 't development', 'intricacy',\n",
       "       'intricacies topic', 'intricacies argument defense',\n",
       "       'intricacies argument', 't devolve', 'intricate details', 'intra',\n",
       "       't dick', 'intimidating', 'interventionists',\n",
       "       'interventionist way possible', 'interventionism',\n",
       "       'interventionary', 'intonation', 'intuitive open',\n",
       "       't determine direction', 't deserve win', 'intrinsicness argument',\n",
       "       't definitely', 'intrinsically tied', 'intrinsically important',\n",
       "       'intrinsic text', 'intrinsic severance perms', 't deter',\n",
       "       'intrinsic permutations', 'intrinsic perm process',\n",
       "       't depends answered', 'intrinsic critiques arguments',\n",
       "       'intrinsic arguments', 'intrinsic argument',\n",
       "       'intrinsic advantages', 't depends', 'intuitive positions',\n",
       "       't defaults', 'intuitive teams', 't debates competing',\n",
       "       't debates difference', 'involve united states', 'involve united',\n",
       "       'involve rely pure', 'involve rely', 'involved college policy',\n",
       "       'involve plans relevant', 'involve lot interaction',\n",
       "       'involve criticism', 'involve case', 'invoke tko',\n",
       "       't debates favorite', 'invocation stand final', 'involve plans',\n",
       "       'invocation stand', 't debates applies', 'involved judging',\n",
       "       't debate things', 'involved type debate', 'involved type',\n",
       "       'involved topic research', 'involved time',\n",
       "       'involved speech debate', 'involved debate really',\n",
       "       'involved speech', 'involved reading', 'involved public forum',\n",
       "       't debate want', 'involved paperless using', 'involved paperless',\n",
       "       'involved k', 't debate vote', 'intervention y comfort',\n",
       "       'inviting', 't debates fun', 'inverse relationship',\n",
       "       'inverse ks read', 'inverse ks', 't deep', 'inventive', 'invent',\n",
       "       'inversely', 'invasion', 'invalidating', 'invading', 'invade',\n",
       "       'intuitively understand', 'intuitively answered',\n",
       "       'intuitive think', 't default judge', 'invites', 'invest lot',\n",
       "       't debates teams', 't debates generally', 'invitational',\n",
       "       't debates good', 't debates high', 'invigorating',\n",
       "       't debates lean', 'invest substantial time', 'investment time',\n",
       "       'investment debate', 't debates lot', 't debates question',\n",
       "       't debates really', 'invested debating', 'invested debate',\n",
       "       'investment framework', 'involves claim', 'intervention y',\n",
       "       'intervention required', 'interpretations topicality debates',\n",
       "       'interpretations typically', 'interpretations unless aff',\n",
       "       'interpretations used', 'interpretations used makes',\n",
       "       'interpretations usually', 'interpretations ve',\n",
       "       'interpretations violations', 't ego', 't effects t',\n",
       "       'interpretative', 'interpreted certain', 'interpreted certain way',\n",
       "       'interpreted description', 'interpreting evidence', 't education',\n",
       "       'interpretive dance', 'interps makes', 'interps lot',\n",
       "       'interps limits x', 'interps limits', 'interps like',\n",
       "       'interps impact', 't endorse', 'interps explain',\n",
       "       'interps counterinterps', 'interps clear', 'interps bad', 't easy',\n",
       "       'interpretive hubris don', 'interpretive hubris', 'interps drop',\n",
       "       't endorse plan', 't enjoy debate', 'interpretations theory aff',\n",
       "       'interpretations plan', 'interpretations persuaded topicality',\n",
       "       'interpretations persuaded reasonability', 't especially enjoy',\n",
       "       'interpretations open', 'interpretations negatives read',\n",
       "       'interpretations predictable', 'interpretations negative wins',\n",
       "       'interpretations naturally competitive',\n",
       "       'interpretations naturally', 'interpretations metric',\n",
       "       't evaluate argument', 'interpretations matter',\n",
       "       'interpretations makes sense', 'interpretations neg',\n",
       "       'interps need', 'interpretations pretty',\n",
       "       'interpretations question', 't enjoy debates',\n",
       "       'interpretations tend points', 't enjoy good',\n",
       "       'interpretations t voting', 't enjoy listening',\n",
       "       'interpretations standard', 'interpretations probably default',\n",
       "       'interpretations specific', 'interpretations resolution prove',\n",
       "       'interpretations reasonability topicality', 't entirely',\n",
       "       'interpretations rarely explanation', 'interpretations rarely',\n",
       "       'interpretations questions', 'interpretations said',\n",
       "       'interps persuaded', 'interps probably', 'interps read', 't disad',\n",
       "       't disagree', 'intervenes', 'intervened', 'intervene weigh',\n",
       "       'intervene way', 'intervening debate rounds', 'intervene stop',\n",
       "       'intervene possible', 'intervene personal inclinations',\n",
       "       't discuss', 'intervene m', 'intervene feel', 't distinct',\n",
       "       'intervene possible deciding', 'intervene analysis',\n",
       "       'intervening ev', 'intervening judge', 'intervention prefer',\n",
       "       'intervention make permutation', 'intervention m',\n",
       "       'intervention likely', 'intervention like', 't dig',\n",
       "       'intervening ev review', 'intervention happen',\n",
       "       'intervention especially putting', 'intervention especially',\n",
       "       'intervention debate', 'intervention comes',\n",
       "       'intervention calling cards', 't direction', 'intervention feel',\n",
       "       'intervention worst', 't doc', 'intersects challenge policy',\n",
       "       'interrupt ask question', 'interrupt ask', 't drop important',\n",
       "       't drop team', 't drop things', 'interrogates', 't drop case',\n",
       "       'interrogated affirmative wouldn', 't drop turns', 't earn',\n",
       "       'interps ve', 'interps unless', 'interps topicality', 'interps s',\n",
       "       'interrogated affirmative', 'intertwined', 'interrupt opponent s',\n",
       "       't draw', 'intersects challenge', 'intersects', 't dock',\n",
       "       'intersectional feminism', 't doing work', 't dont',\n",
       "       'interrupt speech', 'intersect', 'interrupting speeches',\n",
       "       'interrupting opponents mid', 'interrupting opponents',\n",
       "       'interrupting opponent s', 'interrupting opponent',\n",
       "       'interrupting cx', 'interscholastic debate consider',\n",
       "       'involves going', 't debate t', 'involving k', 'issue framing',\n",
       "       'issue generally', 'issue goes', 'issue hand', 'issue high',\n",
       "       't coherent argument', 'issue impact round', 'issue isn',\n",
       "       'issue isn t', 'issue judging', 'issue just reason', 'issue k',\n",
       "       'issue key', 'issue key debating', 'issue like', 'issue listen',\n",
       "       'issue long', 'issue raised', 'issue probably',\n",
       "       'issue presumption', 'issue presented', 'issue persuaded',\n",
       "       'issue outweighs', 'issue feel free', 'issue open',\n",
       "       'issue normative', 't clearly connect', 'issue necessarily',\n",
       "       'issue maybe', 'issue make sure', 't close',\n",
       "       'issue normative reasons', 'issue fast simply', 'issue fast',\n",
       "       'issue explain', 'issue believe purpose', 'issue believe debated',\n",
       "       't compete plan', 'issue ask', 't competed', 'issue ar',\n",
       "       'issue cards', 'issue affs', 't complain', 'israel',\n",
       "       'isolation concert better', 'isolation concert',\n",
       "       't complete sentence', 'isolating debate narrow',\n",
       "       'issue affirmative case', 'issue raised debate', 'issue clear',\n",
       "       'issue counterplans', 'issue especially', 'issue end',\n",
       "       'issue education end', 'issue education', 't come close',\n",
       "       't coming', 'issue conditionality', 't comparative', 'issue did',\n",
       "       'issue defer', 'issue debates', 't compelling reason',\n",
       "       'issue debated', 't compete explicit', 'issue discussion',\n",
       "       'issue rarely', 'issue read', 'issue reading', 'issues certain',\n",
       "       'issues case', 'issues care', 'issues big picture', 'issues big',\n",
       "       'issues best resolved', 'issues choose', 't catch speech',\n",
       "       'issues begin', 'issues basically', 'issues arguments',\n",
       "       'issues annoying involved', 'issues annoying', 'issues addressed',\n",
       "       'issues believe priority', 'issues abuse leniency', 'issues close',\n",
       "       'issues completely', 'issues debate pedagogy',\n",
       "       'issues debate outstanding', 'issues debate demonstrates',\n",
       "       'issues debate debaters', 'issues debate debate',\n",
       "       'issues cross ex', 'issues close debates', 'issues cross',\n",
       "       'issues considering', 'issues considerable time',\n",
       "       'issues considerable', 'issues conditionality topicality',\n",
       "       'issues comprehensive', 'issues completely discrete', 'issues cps',\n",
       "       'isolating debate', 't chance guess', 'issued end',\n",
       "       'issue stimulate social', 'issue stimulate', 't choose',\n",
       "       'issue spec', 'issue shouldn t', 'issue shouldn', 't choice',\n",
       "       'issue said', 'issue round consequence', 't clear link',\n",
       "       't clear s', 'issue resolved', 'issue reasonability',\n",
       "       'issue reason', 't clear idea', 'issued end debate', 'issue tend',\n",
       "       't cheat pet', 't change debate', 'issue won', 'issue winning',\n",
       "       'issue win', 'issue warrants', 'issue voting team', 't cheating',\n",
       "       'issue vote', 't cheap', 'issue uniqueness', 'issue try',\n",
       "       'issue truly', 't cheap shots', 't cheat don', 't changing'],\n",
       "      dtype='<U40')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove phrases spoken by less than 10 Senators.\n",
    "counts_per_author = utils.bincount_2d(author_indices, counts.toarray())\n",
    "min_authors_per_word = 8\n",
    "author_counts_per_word = np.sum(counts_per_author > 0, axis=0)\n",
    "acceptable_words = np.where(\n",
    "    author_counts_per_word >= min_authors_per_word)[0]\n",
    "\n",
    "ranking = author_counts_per_word.argsort()\n",
    "len(vocabulary)\n",
    "# vocabulary[ranking][:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit final document-term matrix with modified vocabulary.\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                   vocabulary=vocabulary[acceptable_words])\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "# Adjust counts by removing unigram/n-gram pairs which co-occur.\n",
    "counts_dense = utils.remove_cooccurring_ngrams(counts, vocabulary)\n",
    "\n",
    "# Remove speeches with not enough words.\n",
    "existing_speeches = np.where(np.sum(counts_dense, axis=1) > 1)[0]\n",
    "counts_dense = counts_dense[existing_speeches]\n",
    "author_indices = author_indices[existing_speeches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239266\n",
      "40653\n",
      "(1817, 239266)\n"
     ]
    }
   ],
   "source": [
    "# no stopword removal, remove words said rarely\n",
    "\n",
    "speaker_to_speaker_id_f = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices_f = np.array(\n",
    "    [speaker_to_speaker_id_f[s] for s in speaker])\n",
    "author_map_f = np.array(list(speaker_to_speaker_id_f.keys()))\n",
    "\n",
    "count_vectorizer_f = CountVectorizer(min_df=0.001,\n",
    "                                   ngram_range=(1, 3),\n",
    "                                   token_pattern=\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "# Learn initial document term matrix. This is only initial because we use it to\n",
    "# identify words to exclude based on author counts.\n",
    "counts_f = count_vectorizer_f.fit_transform(speeches)\n",
    "\n",
    "vocabulary_f = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer_f.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "# Remove phrases spoken by less than 10 Senators.\n",
    "counts_per_author_f = utils.bincount_2d(author_indices_f, counts_f.toarray())\n",
    "min_authors_per_word_f = 8\n",
    "author_counts_per_word_f = np.sum(counts_per_author_f > 0, axis=0)\n",
    "acceptable_words_f = np.where(\n",
    "    author_counts_per_word_f >= min_authors_per_word_f)[0]\n",
    "\n",
    "ranking = author_counts_per_word_f.argsort()\n",
    "print(len(vocabulary_f))\n",
    "print(len(acceptable_words_f))\n",
    "print(counts_f.shape)\n",
    "\n",
    "count_vectorizer_f = CountVectorizer(ngram_range=(1, 3),\n",
    "                                     vocabulary=vocabulary_f[acceptable_words_f]\n",
    "                                   )\n",
    "counts_f = count_vectorizer_f.fit_transform(speeches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_f = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer_f.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "print(len(vocabulary_f))\n",
    "\n",
    "# Adjust counts by removing unigram/n-gram pairs which co-occur.\n",
    "counts_dense_f = utils.remove_cooccurring_ngrams(counts_f, vocabulary_f)\n",
    "\n",
    "# Remove speeches with not enough words.\n",
    "existing_speeches_f = np.where(np.sum(counts_dense_f, axis=1) > 1)[0]\n",
    "counts_dense = counts_dense_f[existing_speeches_f]\n",
    "author_indices_f = author_indices_f[existing_speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9752"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts.npz\"),\n",
    "                sparse.csr_matrix(counts_dense).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices.npy\"), author_indices)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary.txt\"), vocabulary, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map.txt\"), author_map, fmt=\"%s\")\n",
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for document in speeches[existing_speeches]]\n",
    "np.savetxt(os.path.join(save_dir, \"raw_documents.txt\"), \n",
    "           raw_documents, \n",
    "           fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for full list.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts_f.npz\"),\n",
    "                sparse.csr_matrix(counts_dense_f).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices_f.npy\"), author_indices_f)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary_f.txt\"), vocabulary_f, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map_f.txt\"), author_map_f, fmt=\"%s\")\n",
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents_f = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for document in speeches[existing_speeches_f]]\n",
    "np.savetxt(os.path.join(save_dir, \"raw_documents_f.txt\"), \n",
    "           raw_documents_f, \n",
    "           fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#og dataframe\n",
    "df.to_csv(os.path.join(save_dir, 'id_name_para.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
