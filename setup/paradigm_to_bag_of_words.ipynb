{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert Senate speech data from 114th Congress to bag of words format.\n",
    "\n",
    "The data is provided by [1]. Specifically, we use the `hein-daily` data. To \n",
    "run this script, make sure the relevant files are in \n",
    "`data/senate-speeches-114/raw/`. The files needed for this script are \n",
    "`speeches_114.txt`, `descr_114.txt`, and `114_SpeakerMap.txt`.\n",
    "\n",
    "#### References\n",
    "[1]: Gentzkow, Matthew, Jesse M. Shapiro, and Matt Taddy. Congressional Record \n",
    "     for the 43rd-114th Congresses: Parsed Speeches and Phrase Counts. Palo \n",
    "     Alto, CA: Stanford Libraries [distributor], 2018-01-16. \n",
    "     https://data.stanford.edu/congress_text\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import setup_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set params\n",
    "\n",
    "stops = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(\n",
    "    os.path.join(os.path.dirname('.'), os.pardir)) \n",
    "data_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\raw\")\n",
    "save_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\clean\\\\\"+stops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>It's been a number of years since I've been an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>Just a brief update for the high school commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>Please email me your speech documents. I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Maggie Berthiaume</td>\n",
       "      <td>Maggie Berthiaume Woodward Academy Current Coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Bill Russell</td>\n",
       "      <td>Bill Russell Judge Philosophy Overview- I love...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Judge Name                                           Paradigm\n",
       "Judge ID                                                                      \n",
       "3               Aaron Hardy  It's been a number of years since I've been an...\n",
       "1057          Shunta Jordan  Just a brief update for the high school commun...\n",
       "1088            Bill Smelko  Please email me your speech documents. I have ...\n",
       "1265      Maggie Berthiaume  Maggie Berthiaume Woodward Academy Current Coa...\n",
       "1464           Bill Russell  Bill Russell Judge Philosophy Overview- I love..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_markdown_to_plain_text(markdown_string):\n",
    "\n",
    "    markdown_string = str(markdown_string)\n",
    "    # Remove newlines\n",
    "    plain_text = markdown_string.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove bold formatting (e.g., **text**)\n",
    "    plain_text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', plain_text)\n",
    "    \n",
    "    # Remove backslashes\n",
    "    plain_text = plain_text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove other markdown formatting if needed\n",
    "    plain_text = plain_text.replace('\\t', '')\n",
    "    \n",
    "    return plain_text\n",
    "\n",
    "def fix_spaces(string):\n",
    "    new_string = ' '.join(string.strip().split())\n",
    "    return new_string\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'paradigms.csv'), index_col=0)[['Judge Name', 'Paradigm',\"Judge's CEDA rounds\"]]\n",
    "df = df[df[\"Judge's CEDA rounds\"] > 5] #scare away tabroom ghosts\n",
    "df.drop(\"Judge's CEDA rounds\", axis=1, inplace=True)\n",
    "df['Paradigm'] = df['Paradigm'].apply(convert_markdown_to_plain_text)\n",
    "df['Paradigm'] = df['Paradigm'].apply(fix_spaces)\n",
    "df.sort_index(inplace=True)\n",
    "df = df[~df['Judge Name'].isna()]\n",
    "df = df[df['Paradigm'].str.len() > 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1262, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262,)\n",
      "(1262,)\n",
      "(1262,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1262"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "speaker = np.array(df.index.values)\n",
    "\n",
    "speeches = np.array(df['Paradigm'])\n",
    "\n",
    "\n",
    "# Create mapping between names and IDs.\n",
    "speaker_to_speaker_id = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speaker])\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "\n",
    "print(author_map.shape)\n",
    "print(author_indices.shape)\n",
    "print(speaker.shape)\n",
    "len(set(speaker))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'i',\n",
       " 'to',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'you',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'for',\n",
       " 'it',\n",
       " 'debate',\n",
       " 'if',\n",
       " 'not',\n",
       " 'be',\n",
       " 'are',\n",
       " 'on',\n",
       " 'your',\n",
       " 'have',\n",
       " 'me',\n",
       " 'or',\n",
       " 'as',\n",
       " 'arguments',\n",
       " 'will',\n",
       " 'but',\n",
       " 'my',\n",
       " 's',\n",
       " 'with',\n",
       " 'this',\n",
       " 'do',\n",
       " 'of the',\n",
       " 'should',\n",
       " 'if you',\n",
       " 'don',\n",
       " 'don t',\n",
       " 'what',\n",
       " 'an',\n",
       " 'can',\n",
       " 'think',\n",
       " 'in the',\n",
       " 'more',\n",
       " 'am',\n",
       " 'argument',\n",
       " 'at',\n",
       " 'about',\n",
       " 'i am',\n",
       " 'i will',\n",
       " 'debates',\n",
       " 'so',\n",
       " 'round',\n",
       " 'make',\n",
       " 'm',\n",
       " 'i m',\n",
       " 'they',\n",
       " 'just',\n",
       " 'vote',\n",
       " 'how',\n",
       " 'than',\n",
       " 'other',\n",
       " 'to the',\n",
       " 'the debate',\n",
       " 'why',\n",
       " 'by',\n",
       " 'when',\n",
       " 'to be',\n",
       " 'read',\n",
       " 'i think',\n",
       " 'all',\n",
       " 'team',\n",
       " 'judge',\n",
       " 'most',\n",
       " 'some',\n",
       " 'i have',\n",
       " 'on the',\n",
       " 'there',\n",
       " 'their',\n",
       " 'them',\n",
       " 'also',\n",
       " 'is a',\n",
       " 'from',\n",
       " 'out',\n",
       " 'these',\n",
       " 'you are',\n",
       " 'any',\n",
       " 'please',\n",
       " 'well',\n",
       " 'because',\n",
       " 'that i',\n",
       " 're',\n",
       " 'it is',\n",
       " 'need',\n",
       " 'i don',\n",
       " 'i don t',\n",
       " 'one',\n",
       " 'it s',\n",
       " 'no',\n",
       " 'very',\n",
       " 'important',\n",
       " 'for the']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "stopwords = list(\n",
    "    np.loadtxt('stops.txt',\n",
    "               dtype=str,\n",
    "               delimiter=\",\")[0:100,0])\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=0.001,\n",
    "                                   max_df=0.8, \n",
    "                                   stop_words='english', \n",
    "                                   ngram_range=(1, 3),\n",
    "                                   token_pattern=\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "# Learn initial document term matrix. This is only initial because we use it to\n",
    "# identify words to exclude based on author counts.\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove phrases spoken by less than 8 judges.\n",
    "counts_per_author = utils.bincount_2d(author_indices, counts.toarray())\n",
    "min_authors_per_word = 8\n",
    "author_counts_per_word = np.sum(counts_per_author > 0, axis=0)\n",
    "acceptable_words = np.where(\n",
    "    author_counts_per_word >= min_authors_per_word)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s', 'argument', 'good', 'don t', 'don', 'like', 'think', 'make',\n",
       "       'vote', 'debates', 'theory', 'aff', 'impact', 'just', 'judge',\n",
       "       'round', 'm', 'read', 'time', 'team'], dtype='<U40')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ranking = np.flip(np.argsort(author_counts_per_word))\n",
    "ranking\n",
    "p = np.flip(np.sort(author_counts_per_word))\n",
    "p[:20]\n",
    "\n",
    "vocabulary[ranking][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10492"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fit final document-term matrix with modified vocabulary.\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                   vocabulary=vocabulary[acceptable_words])\n",
    "counts = count_vectorizer.fit_transform(speeches)\n",
    "vocabulary = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "# Adjust counts by removing unigram/n-gram pairs which co-occur.\n",
    "counts_dense = utils.remove_cooccurring_ngrams(counts, vocabulary)\n",
    "\n",
    "# Remove speeches with not enough words.\n",
    "existing_speeches = np.where(np.sum(counts_dense, axis=1) > 1)[0]\n",
    "counts_dense = counts_dense[existing_speeches]\n",
    "author_indices = author_indices[existing_speeches]\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts.npz\"),\n",
    "                sparse.csr_matrix(counts_dense).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices.npy\"), author_indices)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary.txt\"), vocabulary, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map.txt\"), author_map, fmt=\"%s\")\n",
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for document in speeches[existing_speeches]]\n",
    "np.savetxt(os.path.join(save_dir, \"raw_documents.txt\"), \n",
    "           raw_documents, \n",
    "           fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#og dataframe\n",
    "df.to_csv(os.path.join(save_dir, 'id_name_para.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
