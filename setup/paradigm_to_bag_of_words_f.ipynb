{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert Senate speech data from 114th Congress to bag of words format.\n",
    "\n",
    "The data is provided by [1]. Specifically, we use the `hein-daily` data. To \n",
    "run this script, make sure the relevant files are in \n",
    "`data/senate-speeches-114/raw/`. The files needed for this script are \n",
    "`speeches_114.txt`, `descr_114.txt`, and `114_SpeakerMap.txt`.\n",
    "\n",
    "#### References\n",
    "[1]: Gentzkow, Matthew, Jesse M. Shapiro, and Matt Taddy. Congressional Record \n",
    "     for the 43rd-114th Congresses: Parsed Speeches and Phrase Counts. Palo \n",
    "     Alto, CA: Stanford Libraries [distributor], 2018-01-16. \n",
    "     https://data.stanford.edu/congress_text\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import setup_utils as utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(\n",
    "    os.path.join(os.path.dirname('.'), os.pardir)) \n",
    "data_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\raw\")\n",
    "save_dir = os.path.join(project_dir, \"data\\\\paradigms\\\\clean\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judge Name</th>\n",
       "      <th>Paradigm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Judge ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris Palmer</td>\n",
       "      <td>Tabroom.com is mostly my fault. Therefore I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Hardy</td>\n",
       "      <td>It's been a number of years since I've been an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Paul Wexler</td>\n",
       "      <td>Debate Paradigm Paul Wexler Coach since 1993, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Shunta Jordan</td>\n",
       "      <td>Just a brief update for the high school commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Bill Smelko</td>\n",
       "      <td>Please email me your speech documents. I have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Judge Name                                           Paradigm\n",
       "Judge ID                                                                  \n",
       "1          Chris Palmer  Tabroom.com is mostly my fault. Therefore I'm ...\n",
       "3           Aaron Hardy  It's been a number of years since I've been an...\n",
       "319         Paul Wexler  Debate Paradigm Paul Wexler Coach since 1993, ...\n",
       "1057      Shunta Jordan  Just a brief update for the high school commun...\n",
       "1088        Bill Smelko  Please email me your speech documents. I have ..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_markdown_to_plain_text(markdown_string):\n",
    "\n",
    "    markdown_string = str(markdown_string)\n",
    "    # Remove newlines\n",
    "    plain_text = markdown_string.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove bold formatting (e.g., **text**)\n",
    "    plain_text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', plain_text)\n",
    "    \n",
    "    # Remove backslashes\n",
    "    plain_text = plain_text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove other markdown formatting if needed\n",
    "    plain_text = plain_text.replace('\\t', '')\n",
    "    \n",
    "    return plain_text\n",
    "\n",
    "def fix_spaces(string):\n",
    "    new_string = ' '.join(string.strip().split())\n",
    "    return new_string\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'paradigms.csv'), index_col=0)[['Judge Name', 'Paradigm']]\n",
    "\n",
    "df['Paradigm'] = df['Paradigm'].apply(convert_markdown_to_plain_text)\n",
    "df['Paradigm'] = df['Paradigm'].apply(fix_spaces)\n",
    "df.sort_index(inplace=True)\n",
    "df = df[~df['Judge Name'].isna()]\n",
    "df = df[df['Paradigm'].str.len() > 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1817,)\n",
      "(1817,)\n",
      "(1817,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1817"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "speaker = np.array(df.index.values)\n",
    "\n",
    "speeches = np.array(df['Paradigm'])\n",
    "\n",
    "\n",
    "# Create mapping between names and IDs.\n",
    "speaker_to_speaker_id = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices = np.array(\n",
    "    [speaker_to_speaker_id[s] for s in speaker])\n",
    "author_map = np.array(list(speaker_to_speaker_id.keys()))\n",
    "\n",
    "print(author_map.shape)\n",
    "print(author_indices.shape)\n",
    "print(speaker.shape)\n",
    "len(set(speaker))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239266\n",
      "40653\n",
      "(1817, 239266)\n"
     ]
    }
   ],
   "source": [
    "# no stopword removal, remove words said rarely\n",
    "\n",
    "speaker_to_speaker_id_f = dict(\n",
    "    [(y, x) for x, y in enumerate(speaker)])\n",
    "author_indices_f = np.array(\n",
    "    [speaker_to_speaker_id_f[s] for s in speaker])\n",
    "author_map_f = np.array(list(speaker_to_speaker_id_f.keys()))\n",
    "\n",
    "count_vectorizer_f = CountVectorizer(min_df=0.001,\n",
    "                                   ngram_range=(1, 3),\n",
    "                                   token_pattern=\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "# Learn initial document term matrix. This is only initial because we use it to\n",
    "# identify words to exclude based on author counts.\n",
    "counts_f = count_vectorizer_f.fit_transform(speeches)\n",
    "\n",
    "vocabulary_f = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer_f.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "# Remove phrases spoken by less than 10 Senators.\n",
    "counts_per_author_f = utils.bincount_2d(author_indices_f, counts_f.toarray())\n",
    "min_authors_per_word_f = 8\n",
    "author_counts_per_word_f = np.sum(counts_per_author_f > 0, axis=0)\n",
    "acceptable_words_f = np.where(\n",
    "    author_counts_per_word_f >= min_authors_per_word_f)[0]\n",
    "\n",
    "ranking = author_counts_per_word_f.argsort()\n",
    "print(len(vocabulary_f))\n",
    "print(len(acceptable_words_f))\n",
    "print(counts_f.shape)\n",
    "\n",
    "count_vectorizer_f = CountVectorizer(ngram_range=(1, 3),\n",
    "                                     vocabulary=vocabulary_f[acceptable_words_f]\n",
    "                                   )\n",
    "counts_f = count_vectorizer_f.fit_transform(speeches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_f = np.array(\n",
    "    [k for (k, v) in sorted(count_vectorizer_f.vocabulary_.items(), \n",
    "                            key=lambda kv: kv[1])])\n",
    "\n",
    "print(len(vocabulary_f))\n",
    "\n",
    "# Adjust counts by removing unigram/n-gram pairs which co-occur.\n",
    "counts_dense_f = utils.remove_cooccurring_ngrams(counts_f, vocabulary_f)\n",
    "\n",
    "# Remove speeches with not enough words.\n",
    "existing_speeches_f = np.where(np.sum(counts_dense_f, axis=1) > 1)[0]\n",
    "counts_dense = counts_dense_f[existing_speeches_f]\n",
    "author_indices_f = author_indices_f[existing_speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9752"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for full list.\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# `counts.npz` is a [num_documents, num_words] sparse matrix containing the\n",
    "# word counts for each document.\n",
    "sparse.save_npz(os.path.join(save_dir, \"counts_f.npz\"),\n",
    "                sparse.csr_matrix(counts_dense_f).astype(np.float32))\n",
    "# `author_indices.npy` is a [num_documents] vector where each entry is an\n",
    "# integer indicating the author of the corresponding document.\n",
    "np.save(os.path.join(save_dir, \"author_indices_f.npy\"), author_indices_f)\n",
    "# `vocabulary.txt` is a [num_words] vector where each entry is a string\n",
    "# denoting the corresponding word in the vocabulary.\n",
    "np.savetxt(os.path.join(save_dir, \"vocabulary_f.txt\"), vocabulary_f, fmt=\"%s\")\n",
    "# `author_map.txt` is a [num_authors] vector of strings providing the name of\n",
    "# each author in the corpus.\n",
    "np.savetxt(os.path.join(save_dir, \"author_map_f.txt\"), author_map_f, fmt=\"%s\")\n",
    "# `raw_documents.txt` contains all the documents we ended up using.\n",
    "raw_documents_f = [document.replace(\"\\n\", ' ').replace(\"\\r\", ' ') \n",
    "                 for document in speeches[existing_speeches_f]]\n",
    "np.savetxt(os.path.join(save_dir, \"raw_documents_f.txt\"), \n",
    "           raw_documents_f, \n",
    "           fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#og dataframe\n",
    "df.to_csv(os.path.join(save_dir, 'id_name_para.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
